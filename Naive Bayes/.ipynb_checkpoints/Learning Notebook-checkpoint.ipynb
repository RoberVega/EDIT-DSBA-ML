{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a popular machine learning algorithm for classification tasks, such as predicting the class label of a document or an image. It is based on Bayes' theorem, which is a fundamental concept in probability theory.\n",
    "\n",
    "The basic idea behind Naive Bayes is to calculate the probability of each class label given the input features, and then choose the label with the highest probability as the predicted label. To do this, the algorithm makes the assumption that the input features are conditionally independent given the class label, which is why it is called \"naive\". This assumption simplifies the calculation of the probabilities, making Naive Bayes a fast and scalable algorithm that can handle large datasets.\n",
    "\n",
    "Here's a simplified example to illustrate how Naive Bayes works:\n",
    "\n",
    "Suppose we have a dataset of emails that are labeled as either spam or not spam (ham), and we want to classify a new email as either spam or ham. We can represent each email as a bag of words, where each word in the email is a feature. We can then calculate the probability of the email being spam or ham, given the words in the email.\n",
    "\n",
    "Using Bayes' theorem, we can write:\n",
    "\n",
    "$P(spam|words) = \\frac{P(words|spam) \\cdot P(spam)}{P(words)}$\n",
    "\n",
    "$P(ham|words) = \\frac{P(words|ham) \\cdot P(ham)}{P(words)}$\n",
    "\n",
    "where $P(spam|words)$ is the probability of the email being spam given the words in the email, $P(words|spam)$ is the probability of seeing the words in a spam email, $P(spam)$ is the prior probability of an email being spam, and $P(words)$ is the probability of seeing the words in any email (spam or ham). Similarly, $P(ham|words)$ is the probability of the email being ham given the words in the email.\n",
    "\n",
    "To calculate the probabilities, we can use the training data to estimate the probabilities of seeing each word in a spam email and a ham email, as well as the prior probabilities of spam and ham. We can then plug these values into the equations above to calculate the probabilities of the email being spam or ham, and choose the label with the highest probability as the predicted label.\n",
    "\n",
    "This is the basic idea behind Naive Bayes. The algorithm can be extended to handle multiple classes, as well as continuous and categorical features, but the core principle of calculating conditional probabilities based on Bayes' theorem remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6343600637280935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, newsgroups_train.target)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following updated code, we use a TfidfVectorizer instead of a CountVectorizer, which uses TF-IDF weighting instead of simple word counts. We also use a pipeline to combine the vectorization and classification steps, and we experiment with a different alpha value for the Naive Bayes classifier.\n",
    "\n",
    "These modifications should improve the accuracy of the classifier on the 20 Newsgroups dataset. However, keep in mind that the dataset itself is quite challenging, so the accuracy may still be lower than on other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6988847583643123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Define the pipeline\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultinomialNB(alpha=0.1))\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = clf.predict(newsgroups_test.data)\n",
    "accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
