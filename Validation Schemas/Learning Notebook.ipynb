{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Schemas\n",
    "\n",
    "## Holdout Method\n",
    "\n",
    "In this method, we divide the dataset into train, test and validation. Normally, we would use the train and validation schema to choose the best model, while leaving the test for final checks and metrics. For example, when using a random forest and trying to find the best combination of parameters. For this, we use the *train_test_split* function twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "\n",
      "Train: 341\n",
      "Rest: 228\n",
      "\n",
      "\n",
      "Train: 341\n",
      "Validation: 114 \n",
      "Test: 114\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# load the dataset\n",
    "X = load_breast_cancer().data\n",
    "y = load_breast_cancer().target\n",
    "\n",
    "# print its size\n",
    "print(X.shape)\n",
    "\n",
    "# divide the dataset into train and a temporary variable (stratify=y keeps an\n",
    "# equal percentage of the 0 and 1 class)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X,y,test_size=0.4,stratify=y)\n",
    "\n",
    "print(f'\\nTrain: {X_train.shape[0]}\\nRest: {X_temp.shape[0]}\\n\\n')\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp,y_temp,test_size=0.5,stratify=y_temp)\n",
    "\n",
    "print(f'Train: {X_train.shape[0]}\\nValidation: {X_val.shape[0]} \\nTest: {X_test.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-fold Cross-Validation \n",
    "\n",
    "Im this technique a test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into $k$ smaller sets. The following procedure is followed for each of the k “folds”:\n",
    "- A model is trained using $k-1$ of the folds as training data;\n",
    "- The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94202899 0.97058824 0.94117647 0.94117647 0.94117647]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9517543859649122"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,stratify=y)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print(scores)\n",
    "clf.fit(X_train,y_train)\n",
    "accuracy_score(clf.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 5], [0 3]\n",
      "[0 1 3 4], [2 5]\n",
      "[0 2 3 4 5], [1]\n",
      "[0 1 2 3 5], [4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "for train, test in kf.split(X):\n",
    "     print(f\"{train}, {test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out Cross Validation\n",
    "\n",
    "This technique consists on leaving one element out in each of the trainings of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame({'y': [6, 8, 12, 14, 14, 15, 17, 22, 24, 23],\n",
    "                   'x1': [2, 5, 4, 3, 4, 6, 7, 5, 8, 9],\n",
    "                   'x2': [14, 12, 12, 13, 7, 8, 7, 4, 6, 5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.7159581  -4.91317829 -1.37391167 -6.76454294 -3.88354531 -2.18982218\n",
      " -2.22270204 -3.37927664 -4.43852243 -0.5800885 ]\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#define predictor and response variables\n",
    "X = df[['x1', 'x2']]\n",
    "y = df['y']\n",
    "\n",
    "#define cross-validation method to use\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "#build multiple linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "#use LOOCV to evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error',\n",
    "                         cv=cv, n_jobs=-1)\n",
    "\n",
    "print(scores)\n",
    "print(len(scores))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-group-out cross validation\n",
    "\n",
    "\n",
    "Provides train/test indices to split data such that each training set is comprised of all samples except ones belonging to one specific group. Arbitrary domain specific group information is provided an array integers that encodes the group of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[2 3], group=[2 2]\n",
      "  Test:  index=[0 1], group=[1 1]\n",
      "Fold 1:\n",
      "  Train: index=[0 1], group=[1 1]\n",
      "  Test:  index=[2 3], group=[2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 1, 2])\n",
    "\n",
    "# In this method, we define to which group the elements belong to\n",
    "groups = np.array([1, 1, 2, 2])\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(logo.split(X, y, groups)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
    "    print(f\"  Test:  index={test_index}, group={groups[test_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation\n",
    "\n",
    "The k-fold cross-validation procedure is used to estimate the performance of machine learning models when making predictions on data not used during training.\n",
    "\n",
    "This procedure can be used both when optimizing the hyperparameters of a model on a dataset, and when comparing and selecting a model for the dataset. When the same cross-validation procedure and dataset are used to both tune and select a model, it is likely to lead to an optimistically biased evaluation of the model performance.\n",
    "\n",
    "One approach to overcoming this bias is to nest the hyperparameter optimization procedure under the model selection procedure. This is called double cross-validation or nested cross-validation and is the preferred way to evaluate and compare tuned machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.927 (0.020)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1, n_informative=10, n_redundant=10)\n",
    "\n",
    "# configure the inner cross-validation procedure\n",
    "cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# define the model\n",
    "rfc = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# define search space\n",
    "space = {'n_estimators' : [10, 100, 500],\n",
    "         'max_features' : [2, 4, 6]}\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(rfc, space, scoring='accuracy', n_jobs=1, cv=cv_inner, refit=True)\n",
    "\n",
    "# configure the outer cross-validation procedure\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# execute the nested cross-validation\n",
    "scores = cross_val_score(search, X, y, scoring='accuracy', cv=cv_outer, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (pd.Series(scores).mean(), pd.Series(scores).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wilcoxon signed-rank test\n",
    "\n",
    "As Data Scientists, we want to make sure that I understand if a model is actually significantly more accurate than another. Fortunately, many methods exist that apply statistics to the selection of Machine Learning models.\n",
    "\n",
    "The Wilcoxon signed-rank test which is the non-parametric version of the paired Student’s t-test. It can be used when the sample size is small and the data does not follow a normal distribution.\n",
    "\n",
    "We can apply this significance test for comparing two Machine Learning models. Using k-fold cross-validation we can create, for each model, k accuracy scores. This will result in two samples, one for each model.\n",
    "\n",
    "Then, we can use the Wilcoxon signed-rank test to test if the two samples differ significantly from each other. If they do, then one is more accurate than the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rober/.local/lib/python3.7/site-packages/scipy/stats/morestats.py:2967: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6766573217164245"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Load the dataset\n",
    "X = load_iris().data\n",
    "y = load_iris().target\n",
    "\n",
    "# Prepare models and select your CV method\n",
    "model1 = ExtraTreesClassifier()\n",
    "model2 = RandomForestClassifier()\n",
    "kf = KFold(n_splits=20)\n",
    "\n",
    "# Extract results for each model on the same folds\n",
    "results_model1 = cross_val_score(model1, X, y, cv=kf)\n",
    "results_model2 = cross_val_score(model2, X, y, cv=kf)\n",
    "\n",
    "# Calculate p value\n",
    "stat, p = wilcoxon(results_model1, results_model2, zero_method='zsplit'); \n",
    "p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McNemar’s Test\n",
    "\n",
    "McNemar’s test is used to check the extent to which the predictions between one model and another match. This is referred to as the homogeneity of the contingency table. From that table, we can calculate $\\chi^2$ which can be used to compute the p-value:\n",
    "\n",
    "If the p-value is lower than 0.05 we can reject the null hypothesis and see that one model is significantly better than the other.\n",
    "\n",
    "We can use mlxtend package to create the table and calculate the corresponding p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi-squared: 3\n",
      "p-value: 0.5078125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table, mcnemar\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = np.array([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
    "                     0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1])\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = np.array([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
    "                     1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0])\n",
    "\n",
    "# Calculate p value\n",
    "tb = mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "chi2, p = mcnemar(ary=tb, exact=True)\n",
    "\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5x2CV paired t-test\n",
    "\n",
    "The 5x2CV paired t-test is a method often used to compare Machine Learning models due to its strong statistical foundation.\n",
    "\n",
    "The method works as follows. Let’s say we have two classifiers, A and B. We randomly split the data in 50% training and 50% test. Then, we train each model on the training data and compute the difference in accuracy between the models from the test set, called DiffA. Then, the training and test splits are reversed and the difference is calculated again in DiffB.\n",
    "\n",
    "This is repeated five times after which the mean variance of the differences is computed (S²). Then, it is used to calculate the t-statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5623122704704893"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from mlxtend.data import iris_data\n",
    "\n",
    "# Prepare data and clfs\n",
    "X, y = iris_data()\n",
    "clf1 = ExtraTreeClassifier()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "\n",
    "# Calculate p-value\n",
    "t, p = paired_ttest_5x2cv(estimator1=clf1,\n",
    "                          estimator2=clf2,\n",
    "                          X=X, y=y,\n",
    "                          random_seed=1)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
